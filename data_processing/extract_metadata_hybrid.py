"""
í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹±: 1ë‹¨ê³„ Gemini + 2ë‹¨ê³„ ìˆœìˆ˜ ì½”ë“œ

ì‚¬ìš©ë²•:
    python extract_metadata_hybrid.py

íŠ¹ì§•:
    - 1ë‹¨ê³„: Gemini 2.5 Proë¡œ ì „ì²´ txt ë¶„ì„ â†’ ì•ˆê±´ ë¼ì¸ ë§¤í•‘
    - 2ë‹¨ê³„: ìˆœìˆ˜ Python ì½”ë“œë¡œ ë°œì–¸ ì¶”ì¶œ (ë¹ ë¥´ê³  ì•ˆì •ì )
    - ë¹„ìš© ì ˆê°: Gemini 2.5 Flash ë‹¨ê³„ ì œê±°
    - ì†ë„ í–¥ìƒ: 10ë°° ì´ìƒ ë¹ ë¦„
    - ì•ˆì •ì„±: JSON íŒŒì‹± ì˜¤ë¥˜ ì—†ìŒ
"""

import requests
from bs4 import BeautifulSoup, NavigableString
import json
import os
import re
import logging
from datetime import datetime
from pathlib import Path
from google import genai
from google.genai import types
from typing import List, Dict, Optional

# ë¡œê·¸ ì„¤ì •
def setup_logging():
    """ë¡œê·¸ ì„¤ì •"""
    log_dir = Path("logs")
    log_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"parsing_hybrid_{timestamp}.log"

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )

    return logging.getLogger(__name__)

logger = setup_logging()


def extract_text_with_links(element):
    """HTML ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ì™€ ë§í¬ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ"""
    result = []

    for child in element.children:
        if isinstance(child, NavigableString):
            text = str(child)
            if text:
                result.append({"type": "text", "content": text})
        elif child.name == 'a':
            link_text = child.get_text()
            href = child.get('href', '')

            if href.startswith('/'):
                full_url = f"https://ms.smc.seoul.kr{href}"
            elif href.startswith('http'):
                full_url = href
            else:
                full_url = href

            result.append({"type": "link", "text": link_text, "url": full_url})
        elif child.name == 'br':
            result.append({"type": "text", "content": "\n"})
        elif child.name == 'hr':
            result.append({"type": "separator", "content": "---"})
        else:
            result.extend(extract_text_with_links(child))

    return result


def extract_reference_materials(content_list):
    """
    (ì°¸ê³ ) ì„¹ì…˜ì—ì„œ ì°¸ê³ ìë£Œ ë§í¬ ì¶”ì¶œ

    Returns:
        [{"title": "ë¬¸ì„œëª…", "url": "ë‹¤ìš´ë¡œë“œ URL"}]
    """
    attachments = []
    in_reference = False
    pending_links = []

    for i, item in enumerate(content_list):
        # (ì°¸ê³ ) ì‹œì‘ ê°ì§€
        if item.get("type") == "text" and "(ì°¸ê³ )" in item.get("content", ""):
            in_reference = True
            pending_links = []
            continue

        # (íšŒì˜ë¡ ëì— ì‹¤ìŒ) ê°ì§€ ì‹œ ì¢…ë£Œ
        if in_reference and item.get("type") == "text" and "íšŒì˜ë¡ ëì— ì‹¤ìŒ" in item.get("content", ""):
            in_reference = False
            # pending_linksë¥¼ attachmentsì— ì¶”ê°€
            attachments.extend(pending_links)
            pending_links = []
            continue

        # (ì°¸ê³ ) êµ¬ê°„ ë‚´ì˜ ë§í¬ ìˆ˜ì§‘
        if in_reference and item.get("type") == "link":
            # PDF/HWP ë‹¤ìš´ë¡œë“œ ë§í¬ë§Œ ì¶”ì¶œ
            url = item.get("url", "")
            if "appendixDownload" in url or url.endswith(('.pdf', '.hwp', '.docx')):
                pending_links.append({
                    "title": item.get("text", "").strip(),
                    "url": url
                })

    return attachments


def crawl_url(url: str) -> Dict:
    """URL í¬ë¡¤ë§í•˜ì—¬ txt í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
    logger.info(f"í¬ë¡¤ë§ ì‹œì‘: {url}")
    print(f"ğŸŒ í¬ë¡¤ë§ ì‹œì‘: {url}\n")

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    response = requests.get(url, headers=headers, timeout=30)
    response.encoding = 'utf-8'

    if response.status_code != 200:
        raise Exception(f"HTTP {response.status_code}")

    soup = BeautifulSoup(response.text, 'html.parser')
    canvas = soup.find('div', id='canvas')

    if not canvas:
        raise Exception("ë©”ì¸ ì»¨í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    title = soup.title.string if soup.title else "ì œëª© ì—†ìŒ"
    title = title.strip()

    # í…ìŠ¤íŠ¸ + ë§í¬ ì¶”ì¶œ
    extracted = extract_text_with_links(canvas)

    # ì²¨ë¶€ ë¬¸ì„œ ì¶”ì¶œ (ì°¸ê³  ì„¹ì…˜ì—ì„œ)
    attachments = extract_reference_materials(extracted)

    # txt ë³€í™˜
    lines = []
    for item in extracted:
        if item['type'] == 'text':
            text = item['content'].strip()
            if text:
                lines.append(text)
        elif item['type'] == 'separator':
            lines.append('---')

    txt_content = '\n'.join(lines)

    # ì°¸ê³ ìë£Œ ì œê±°
    if '(íšŒì˜ë¡ ëì— ì‹¤ìŒ)' in txt_content:
        txt_content = txt_content.split('(íšŒì˜ë¡ ëì— ì‹¤ìŒ)')[0]
    if '(ì°¸ê³ )' in txt_content:
        txt_content = txt_content.split('(ì°¸ê³ )')[0]

    logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ: {len(txt_content)} bytes, ì²¨ë¶€ {len(attachments)}ê°œ")
    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(txt_content):,} bytes\n")
    print(f"ğŸ“ ì²¨ë¶€ ë¬¸ì„œ: {len(attachments)}ê°œ\n")

    return {
        "title": title,
        "url": url,
        "content": txt_content,
        "attachments": attachments
    }


def extract_agenda_mapping(
    txt_content: str,
    title: str,
    url: str,
    api_key: str,
    attachments: List[Dict] = None,
    model: str = "gemini-2.5-pro"
) -> Dict:
    """
    1ë‹¨ê³„: Geminië¡œ ì•ˆê±´ ë¼ì¸ ë§¤í•‘ ì¶”ì¶œ

    Args:
        txt_content: íšŒì˜ë¡ í…ìŠ¤íŠ¸
        title: íšŒì˜ë¡ ì œëª©
        url: íšŒì˜ë¡ URL
        api_key: Google API Key
        attachments: ì²¨ë¶€ ë¬¸ì„œ ëª©ë¡ [{"title": "...", "url": "..."}]
        model: ì‚¬ìš©í•  ëª¨ë¸

    Returns:
        {
            "meeting_info": {
                "title": "...",
                "url": "...",
                "date": "YYYY.MM.DD"
            },
            "agenda_mapping": [
                {
                    "agenda_title": "ì•ˆê±´ ì œëª©",
                    "line_start": 1,
                    "line_end": 50,
                    "speakers": ["ë°œì–¸ì1", "ë°œì–¸ì2"],
                    "attachments": [{"title": "...", "url": "..."}]
                },
                ...
            ]
        }
    """
    logger.info("1ë‹¨ê³„: ì•ˆê±´ ë¼ì¸ ë§¤í•‘ ì¶”ì¶œ ì‹œì‘")
    print("=" * 80)
    print("1ë‹¨ê³„: ì•ˆê±´ ë¼ì¸ ë§¤í•‘ ì¶”ì¶œ (Gemini 2.5 Pro)")
    print("=" * 80)
    print()

    client = genai.Client(api_key=api_key)

    # ë¼ì¸ ë²ˆí˜¸ ì¶”ê°€
    lines = txt_content.split('\n')
    numbered_text = ""
    for i, line in enumerate(lines, 1):
        numbered_text += f"{i:4d} | {line}\n"

    # ì²¨ë¶€ ë¬¸ì„œ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Geminiì—ê²Œ ì „ë‹¬)
    attachments_text = ""
    if attachments:
        attachments_text = "\n\nì²¨ë¶€ ë¬¸ì„œ ëª©ë¡:\n"
        for idx, att in enumerate(attachments, 1):
            attachments_text += f"{idx}. {att['title']} (URL: {att['url']})\n"

    prompt = f"""ë‹¤ìŒì€ ì„œìš¸ì‹œì˜íšŒ íšŒì˜ë¡ì…ë‹ˆë‹¤. ì´ íšŒì˜ë¡ì„ ë¶„ì„í•˜ì—¬ ì•ˆê±´ë³„ ë¼ì¸ ë²ˆí˜¸ ë§¤í•‘ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

íšŒì˜ë¡ ì œëª©: {title}
íšŒì˜ë¡ URL: {url}
{attachments_text}

íšŒì˜ë¡ ë‚´ìš© (ë¼ì¸ ë²ˆí˜¸ í¬í•¨):
{numbered_text}

ì‘ì—…:
1. meeting_info ì¶”ì¶œ:
   - meeting_url: ìœ„ì— ì œê³µëœ "íšŒì˜ë¡ URL"ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
   - date: ì œëª©ì—ì„œ ë‚ ì§œ ì¶”ì¶œ (YYYY.MM.DD í˜•ì‹)

2. agenda_mapping ì¶”ì¶œ:

**ì•ˆê±´ ì‹ë³„ ê·œì¹™:**

1ë‹¨ê³„: "ì‹¬ì‚¬ëœì•ˆê±´" ë˜ëŠ” "ì˜ì‚¬ì¼ì •" ì„¹ì…˜ì—ì„œ ì•ˆê±´ ëª©ë¡ ì¶”ì¶œ
   - "1. 2. 3. ..." í˜•íƒœì˜ ì•ˆê±´ ëª©ë¡ì„ ëª¨ë‘ ì°¾ìœ¼ì„¸ìš”
   - agenda_titleì€ ë²ˆí˜¸ë¥¼ ì œì™¸í•œ ìˆœìˆ˜ ì•ˆê±´ëª…ë§Œ ì‚¬ìš©
   - ì˜ˆ: "1. ê¸°íšì¡°ì •ì‹¤ í˜„ì•ˆ ì—…ë¬´ë³´ê³ " â†’ "ê¸°íšì¡°ì •ì‹¤ í˜„ì•ˆ ì—…ë¬´ë³´ê³ "

2ë‹¨ê³„: ê° ì•ˆê±´ì˜ ë…¼ì˜ êµ¬ê°„ì„ ê°œë³„ì ìœ¼ë¡œ ì§€ì •
   - "---" êµ¬ë¶„ì„ , "â—‹ìœ„ì›ì¥" ë°œì–¸, ì•ˆê±´ëª… ì–¸ê¸‰ì„ ì°¸ê³ í•˜ì—¬ line_start, line_end ì§€ì •
   - **ì¼ê´„ ìƒì •ëœ ì•ˆê±´ë„ ë³¸ë¬¸ ë¶„ì„ìœ¼ë¡œ ê°ê° ë¶„ë¦¬** (ì˜ˆ: "ì œ1í•­, ì œ2í•­ ì¼ê´„ ìƒì •" â†’ ê°œë³„ êµ¬ê°„ ì°¾ê¸°)

**ì•ˆê±´ ì™¸ ì„¹ì…˜:**
- ì•ˆê±´ ëª©ë¡ì— ì—†ì–´ë„ ì‹¤ì œ íšŒì˜ ë‚´ìš©(ê°œì˜, ì§ˆì˜ì‘ë‹µ, 5ë¶„ììœ ë°œì–¸, ì‚°íšŒ ë“±)ì€ ëª¨ë‘ í¬í•¨
- ì„¹ì…˜ ì„±ê²©ì— ë§ëŠ” ì ì ˆí•œ ì œëª© ì‚¬ìš©

**ì²¨ë¶€ ë¬¸ì„œ ë§¤ì¹­:**
- íšŒì˜ë¡ ë³¸ë¬¸ì—ì„œ ë§ˆí¬ë‹¤ìš´ ë§í¬ í˜•ì‹ì˜ ì²¨ë¶€ ë¬¸ì„œë¥¼ ì°¾ì•„ì„œ ë§¤ì¹­
  * í˜•ì‹: [ë¬¸ì„œëª…](https://ms.smc.seoul.kr/record/appendixDownload.do?key=...)
  * (ì°¸ê³ ) ì„¹ì…˜ì˜ ë§í¬ëŠ” ë°”ë¡œ ì§ì „ ì•ˆê±´ì— ì†í•¨
- ê° ì•ˆê±´ì— í•´ë‹¹í•˜ëŠ” ì²¨ë¶€ ë¬¸ì„œë¥¼ attachments í•„ë“œì— ë°°ì—´ë¡œ ì¶”ê°€
- ì•ˆê±´ê³¼ ê´€ë ¨ ì—†ëŠ” ì²¨ë¶€ ë¬¸ì„œëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ
- ì²¨ë¶€ ë¬¸ì„œê°€ ì—†ëŠ” ì•ˆê±´ì€ attachmentsë¥¼ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
- **ì¤‘ìš”**: URLì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í¬í•¨í•˜ê³ , URLì´ ì—†ìœ¼ë©´ nullë¡œ ì„¤ì •í•˜ì§€ ë§ê³  í•´ë‹¹ í•­ëª©ì„ ì œì™¸

**ì¤‘ìš”:**
- ëª¨ë“  ì•ˆê±´ì„ ë¹ ì§ì—†ì´ í¬í•¨ (ë‹¨ í•˜ë‚˜ë„ ëˆ„ë½ ê¸ˆì§€)
- "(íšŒì˜ë¡ ëì— ì‹¤ìŒ)" ë˜ëŠ” "(ì°¸ê³ )" ì„¹ì…˜ì€ ì œì™¸
- speakers: í•´ë‹¹ êµ¬ê°„ì˜ ë°œì–¸ì ëª©ë¡ (â—‹ ë‹¤ìŒ ì´ë¦„, ë°œì–¸ ìˆœì„œëŒ€ë¡œ)

**ì•ˆê±´ íƒ€ì… ë¶„ë¥˜:**
ê° ì•ˆê±´ì— agenda_type í•„ë“œë¥¼ ì¶”ê°€í•˜ì—¬ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜:
- "legislation": ì¡°ë¡€ì•ˆ, ê·œì¹™ì•ˆ ë“± ì…ë²• ì•ˆê±´
- "report": ì—…ë¬´ë³´ê³ , í˜„ì•ˆë³´ê³  ë“± ë³´ê³  ì•ˆê±´
- "budget": ì˜ˆì‚°ì•ˆ, ê²°ì‚° ê´€ë ¨ ì•ˆê±´
- "consent": ë™ì˜ì•ˆ, ìŠ¹ì¸ì•ˆ, ì˜ê²¬ì²­ì·¨ ì•ˆê±´
- "procedural": ê°œì˜, ê°œíšŒ, ííšŒ, ì‚°íšŒ ë“± ì ˆì°¨ì  ì•ˆê±´
- "personnel": ìœ„ì›ì¥ ì„ ê±°, ìœ„ì› ì„ ì„ ë“± ì¸ì‚¬ ì•ˆê±´
- "discussion": ì§ˆì˜ì‘ë‹µ, 5ë¶„ììœ ë°œì–¸ ë“± í† ë¡ 
- "other": ê¸°íƒ€

**ì•ˆê±´ ìƒíƒœ ì¶”ì¶œ:**
ê° ì•ˆê±´ì˜ ì²˜ë¦¬ ìƒíƒœë¥¼ íŒŒì•…í•˜ì—¬ status í•„ë“œì— ê¸°ë¡:
- íšŒì˜ë¡ì—ì„œ í•´ë‹¹ ì•ˆê±´ì´ ì–´ë–»ê²Œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ ì°¾ì•„ì„œ ê¸°ë¡
- ê°€ëŠ¥í•œ ìƒíƒœ ê°’:
  * "ì›ì•ˆê°€ê²°": "ì›ì•ˆê°€ê²°", "ê°€ê²°ë˜ì—ˆìŒ", "ì˜ê²°ë˜ì—ˆìŒ" ë“±
  * "ìˆ˜ì •ê°€ê²°": "ìˆ˜ì •ê°€ê²°", "ì¼ë¶€ ìˆ˜ì •í•˜ì—¬ ê°€ê²°" ë“±
  * "ë¶€ê²°": "ë¶€ê²°ë˜ì—ˆìŒ"
  * "ë³¸íšŒì˜ ìƒì •": "ë³¸íšŒì˜ì— ë¶€ì˜", "ë³¸íšŒì˜ ìƒì •" ë“±
  * "ìœ„ì›íšŒ ì‹¬ì‚¬ì¤‘": "ìœ„ì›íšŒì— ìƒì •", "ì‹¬ì‚¬ ì¤‘" ë“±
  * "ì ‘ìˆ˜": ìƒíƒœê°€ ëª…ì‹œë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’
- íšŒì˜ë¡ì— ëª…ì‹œëœ í‘œí˜„ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì˜ˆ: "ì›ì•ˆê°€ê²°ë˜ì—ˆìŒì„ ì„ í¬í•©ë‹ˆë‹¤" â†’ "ì›ì•ˆê°€ê²°")
- ìƒíƒœê°€ ë¶ˆëª…í™•í•˜ë©´ "ì ‘ìˆ˜" ì‚¬ìš©

JSON ì¶œë ¥ í˜•ì‹:
{{{{
  "meeting_info": {{{{
    "title": "{title}",
    "meeting_url": "{url}",
    "date": "YYYY.MM.DD"
  }}}},
  "agenda_mapping": [
    {{{{
      "agenda_title": "ì•ˆê±´ëª…",
      "agenda_type": "legislation",
      "status": "ì›ì•ˆê°€ê²°",
      "line_start": 1,
      "line_end": 50,
      "speakers": ["ë°œì–¸ì1", "ë°œì–¸ì2"],
      "attachments": [
        {{{{"title": "ë¬¸ì„œëª…", "download_url": "ì²¨ë¶€íŒŒì¼ ë‹¤ìš´ë¡œë“œ URL"}}}}
      ]
    }}}}
  ]
}}}}

ê·œì¹™:
- ìˆœìˆ˜ JSONë§Œ ì¶œë ¥
- agenda_mappingì€ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë°°ì—´
- line_start, line_endëŠ” ì‹¤ì œ ë¼ì¸ ë²ˆí˜¸ ì‚¬ìš©
- attachmentsëŠ” í•´ë‹¹ ì•ˆê±´ì— ì†í•œ ì²¨ë¶€ ë¬¸ì„œë§Œ í¬í•¨ (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)
- agenda_typeì€ ë°˜ë“œì‹œ ìœ„ 8ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜
- statusëŠ” íšŒì˜ë¡ì—ì„œ ì¶”ì¶œí•œ ì‹¤ì œ ì²˜ë¦¬ ìƒíƒœ (ì—†ìœ¼ë©´ "ì ‘ìˆ˜")
"""

    response = client.models.generate_content(
        model=model,
        contents=prompt,
        config=types.GenerateContentConfig(
            temperature=0.1,
            response_mime_type="application/json",
        )
    )

    response_text = None
    try:
        response_text = response.text
    except:
        pass

    if not response_text:
        if response.candidates and len(response.candidates) > 0:
            candidate = response.candidates[0]
            if hasattr(candidate, 'content') and candidate.content:
                if hasattr(candidate.content, 'parts') and candidate.content.parts:
                    if len(candidate.content.parts) > 0:
                        part = candidate.content.parts[0]
                        if hasattr(part, 'text'):
                            response_text = part.text

    if not response_text:
        raise Exception("ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    result = json.loads(response_text)

    # í† í° ì •ë³´
    tokens = {"input": 0, "output": 0}
    if hasattr(response, 'usage_metadata'):
        tokens["input"] = getattr(response.usage_metadata, 'prompt_token_count', 0)
        tokens["output"] = getattr(response.usage_metadata, 'candidates_token_count', 0)

    logger.info(f"1ë‹¨ê³„ ì™„ë£Œ: {len(result['agenda_mapping'])}ê°œ ì•ˆê±´, í† í°={tokens['input']}+{tokens['output']}")
    print(f"âœ… ì•ˆê±´ ë§¤í•‘ ì¶”ì¶œ ì™„ë£Œ: {len(result['agenda_mapping'])}ê°œ")
    print(f"ğŸ“Š í† í°: input={tokens['input']:,}, output={tokens['output']:,}")
    print()

    return result, tokens


def parse_speaker_line(line: str) -> tuple:
    """
    ë°œì–¸ì ë¼ì¸ íŒŒì‹±

    ì˜ˆ: "â—‹ì˜ì¥ ìµœí˜¸ì •  ì•ˆë…•í•˜ì„¸ìš”." â†’ ("ì˜ì¥ ìµœí˜¸ì •", "ì•ˆë…•í•˜ì„¸ìš”.")
    ì˜ˆ: "â—‹ìœ„ì›ì¥ [ì„œìƒì—´](url)  ì•ˆë…•í•˜ì„¸ìš”." â†’ ("ìœ„ì›ì¥ ì„œìƒì—´", "ì•ˆë…•í•˜ì„¸ìš”.")
    """
    # ë§ˆí¬ë‹¤ìš´ ë§í¬ ì œê±° í•¨ìˆ˜
    def remove_markdown_links(text: str) -> str:
        # [í…ìŠ¤íŠ¸](url) â†’ í…ìŠ¤íŠ¸
        return re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)

    # â—‹ ë‹¤ìŒ ê³µë°± ì œê±°í•˜ê³  íŒŒì‹±
    match = re.match(r'^â—‹\s*(.+?)\s{2,}(.+)$', line)
    if match:
        speaker = remove_markdown_links(match.group(1).strip())
        text = match.group(2).strip()
        return speaker, text

    # ë°œì–¸ìë§Œ ìˆëŠ” ê²½ìš° (ë‹¤ìŒ ì¤„ë¶€í„° ë‚´ìš©)
    match = re.match(r'^â—‹\s*(.+)$', line)
    if match:
        speaker = remove_markdown_links(match.group(1).strip())
        return speaker, ""

    return None, None


def split_long_text(text: str, max_length: int = 500) -> List[str]:
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 

    Args:
        text: ë¶„í• í•  í…ìŠ¤íŠ¸
        max_length: ìµœëŒ€ ê¸¸ì´

    Returns:
        ë¶„í• ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
    """
    if len(text) <= max_length:
        return [text]

    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    sentences = re.split(r'([.?!])\s+', text)

    chunks = []
    current_chunk = ""

    for i in range(0, len(sentences), 2):
        sentence = sentences[i]
        if i + 1 < len(sentences):
            sentence += sentences[i + 1]  # ë§ˆì¹¨í‘œ ì¶”ê°€

        if len(current_chunk) + len(sentence) > max_length:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence
        else:
            current_chunk += " " + sentence

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks if chunks else [text]


def parse_section_pure(section_text: str, agenda_title: str, speakers: List[str], previous_speaker: str = None) -> List[Dict]:
    """
    ìˆœìˆ˜ ì½”ë“œë¡œ ì„¹ì…˜ íŒŒì‹±

    Args:
        section_text: íšŒì˜ë¡ í…ìŠ¤íŠ¸
        agenda_title: ì•ˆê±´ëª…
        speakers: ë°œì–¸ì ëª©ë¡ (1ë‹¨ê³„ì—ì„œ ì œê³µ)
        previous_speaker: ì´ì „ êµ¬ê°„ì˜ ë§ˆì§€ë§‰ ë°œì–¸ì (ë°œì–¸ì ì—†ì„ ë•Œ ì‚¬ìš©)

    Returns:
        chunks ë¦¬ìŠ¤íŠ¸
    """
    chunks = []
    lines = section_text.split('\n')

    current_speaker = previous_speaker  # ì´ì „ ë°œì–¸ìë¡œ ì´ˆê¸°í™”
    current_text_lines = []

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # â—‹ë¡œ ì‹œì‘í•˜ëŠ” ë°œì–¸ì ë¼ì¸ì¸ì§€ í™•ì¸
        if line.startswith('â—‹'):
            # ì´ì „ ë°œì–¸ ì €ì¥
            if current_speaker and current_text_lines:
                full_text = ' '.join(current_text_lines).strip()

                # 500ì ë„˜ìœ¼ë©´ ë¶„í• 
                text_chunks = split_long_text(full_text, max_length=500)

                for text_chunk in text_chunks:
                    chunks.append({
                        "speaker": current_speaker,
                        "agenda": agenda_title,
                        "text": text_chunk
                    })

            # ìƒˆ ë°œì–¸ì ì‹œì‘
            speaker, first_text = parse_speaker_line(line)

            if speaker:
                current_speaker = speaker
                current_text_lines = [first_text] if first_text else []
        else:
            # ë°œì–¸ ë‚´ìš© ê³„ì†
            if current_speaker:
                current_text_lines.append(line)

    # ë§ˆì§€ë§‰ ë°œì–¸ ì €ì¥
    if current_speaker and current_text_lines:
        full_text = ' '.join(current_text_lines).strip()
        text_chunks = split_long_text(full_text, max_length=500)

        for text_chunk in text_chunks:
            chunks.append({
                "speaker": current_speaker,
                "agenda": agenda_title,
                "text": text_chunk
            })

    return chunks


def parse_with_pure_code(txt_content: str, agenda_mapping: List[Dict]) -> List[Dict]:
    """
    2ë‹¨ê³„: ìˆœìˆ˜ ì½”ë“œë¡œ ë°œì–¸ ì¶”ì¶œ

    Args:
        txt_content: ì›ë³¸ txt ë‚´ìš© (í—¤ë” ì œê±° ì „)
        agenda_mapping: 1ë‹¨ê³„ ê²°ê³¼ (ì•ˆê±´ ë§¤í•‘)

    Returns:
        ëª¨ë“  chunks
    """
    logger.info("2ë‹¨ê³„: ìˆœìˆ˜ ì½”ë“œë¡œ ë°œì–¸ ì¶”ì¶œ ì‹œì‘")
    print("=" * 80)
    print("2ë‹¨ê³„: ìˆœìˆ˜ ì½”ë“œë¡œ ë°œì–¸ ì¶”ì¶œ")
    print("=" * 80)
    print()

    # í—¤ë” ì œê±° (=== ì´í›„ë¶€í„°)
    lines = txt_content.split('\n')
    separator_index = -1
    for i, line in enumerate(lines):
        if '=' * 80 in line:
            separator_index = i
            break

    if separator_index != -1:
        lines = lines[separator_index + 1:]

    all_chunks = []
    last_speaker = None  # ì´ì „ ë°œì–¸ì ì¶”ì 

    for idx, agenda in enumerate(agenda_mapping, 1):
        agenda_title = agenda['agenda_title']
        line_start = agenda['line_start'] - 1  # 0-indexed
        line_end = agenda['line_end']
        speakers = agenda.get('speakers', [])

        # ë¼ì¸ ë²”ìœ„ ì¶”ì¶œ
        section_lines = lines[line_start:line_end]
        section_text = '\n'.join(section_lines)

        # íŒŒì‹± (ì´ì „ ë°œì–¸ì ì „ë‹¬)
        chunks = parse_section_pure(section_text, agenda_title, speakers, last_speaker)

        # ì²­í¬ê°€ ìˆìœ¼ë©´ ë§ˆì§€ë§‰ ë°œì–¸ì ì—…ë°ì´íŠ¸
        if chunks:
            last_speaker = chunks[-1]['speaker']

        msg = f"[{idx}/{len(agenda_mapping)}] {len(chunks)}ê°œ ë°œì–¸ ì¶”ì¶œ: {agenda_title[:50]}..."
        logger.info(msg)
        print(f"  âœ“ {msg}")

        all_chunks.extend(chunks)

    logger.info(f"2ë‹¨ê³„ ì™„ë£Œ: ì´ {len(all_chunks)}ê°œ ë°œì–¸ ì¶”ì¶œ")
    print()
    print(f"âœ… ì´ {len(all_chunks)}ê°œ ë°œì–¸ ì¶”ì¶œ ì™„ë£Œ!")
    print()

    return all_chunks


def extract_metadata_from_url(
    url: str,
    api_key: str,
    stage1_model: str = "gemini-2.5-pro",
    verbose: bool = True
) -> Dict:
    """
    URLì—ì„œ ì§ì ‘ í¬ë¡¤ë§ + í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± (ì²¨ë¶€ ë¬¸ì„œ í¬í•¨)

    Args:
        url: íšŒì˜ë¡ URL
        api_key: Google API Key
        stage1_model: 1ë‹¨ê³„ ëª¨ë¸ (ê¸°ë³¸: gemini-2.5-pro)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸: True)

    Returns:
        {
            "meeting_info": {...},
            "chunks": [...],
            "usage": {...}
        }
    """
    logger.info(f"URL ê¸°ë°˜ íŒŒì‹± ì‹œì‘: {url}")

    if verbose:
        print("=" * 100)
        print("í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹±: URL í¬ë¡¤ë§ + 1ë‹¨ê³„ Gemini + 2ë‹¨ê³„ ìˆœìˆ˜ ì½”ë“œ")
        print("=" * 100)
        print()

    # URL í¬ë¡¤ë§
    crawled_data = crawl_url(url)
    txt_content = crawled_data['content']
    title = crawled_data['title']
    attachments = crawled_data.get('attachments', [])

    if verbose:
        print(f"ğŸ“„ ì œëª©: {title}")
        print(f"ğŸ“ í¬ê¸°: {len(txt_content):,} bytes")
        print(f"ğŸ“ ì²¨ë¶€: {len(attachments)}ê°œ")
        print()

    # 1ë‹¨ê³„: ì•ˆê±´ ë§¤í•‘ ì¶”ì¶œ (Gemini)
    import sys
    from io import StringIO
    old_stdout = sys.stdout
    if not verbose:
        sys.stdout = StringIO()

    stage1_result, tokens = extract_agenda_mapping(txt_content, title, url, api_key, attachments, stage1_model)

    if not verbose:
        sys.stdout = old_stdout

    # 2ë‹¨ê³„: ë°œì–¸ ì¶”ì¶œ (ìˆœìˆ˜ ì½”ë“œ)
    if not verbose:
        sys.stdout = StringIO()

    # txt_contentë¥¼ full formatìœ¼ë¡œ ì¬êµ¬ì„± (í—¤ë” í¬í•¨)
    full_content = f"ì œëª©: {title}\nURL: {url}\n{'=' * 80}\n\n{txt_content}"
    chunks = parse_with_pure_code(full_content, stage1_result['agenda_mapping'])

    if not verbose:
        sys.stdout = old_stdout

    # ìµœì¢… ê²°ê³¼ (agenda_mapping í¬í•¨)
    final_result = {
        "meeting_info": stage1_result['meeting_info'],
        "agenda_mapping": stage1_result['agenda_mapping'],  # â­ ì¶”ê°€: ì²¨ë¶€ ë¬¸ì„œ ì •ë³´
        "chunks": chunks,
        "usage": {
            "stage1_model": stage1_model,
            "stage2_method": "pure_python_code",
            "stage1_tokens": tokens,
            "total_chunks": len(chunks)
        }
    }

    logger.info(f"URL ê¸°ë°˜ íŒŒì‹± ì™„ë£Œ: {len(chunks)}ê°œ ë°œì–¸, {len(attachments)}ê°œ ì²¨ë¶€")

    if verbose:
        print("=" * 100)
        print("âœ… URL ê¸°ë°˜ íŒŒì‹± ì™„ë£Œ!")
        print("=" * 100)
        print(f"ì´ ë°œì–¸ ìˆ˜: {len(chunks)}ê°œ")
        print(f"ì´ ì²¨ë¶€ ìˆ˜: {len(attachments)}ê°œ")
        print(f"Stage 1 í† í°: {tokens['input']:,} + {tokens['output']:,}")
        print(f"Stage 2 ë°©ì‹: ìˆœìˆ˜ Python ì½”ë“œ (ë¹„ìš© 0ì›)")
        print()

    return final_result


def extract_metadata_hybrid(
    txt_path: str,
    api_key: str,
    stage1_model: str = "gemini-2.5-pro",
    verbose: bool = True
) -> Dict:
    """
    í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹±: 1ë‹¨ê³„ Gemini + 2ë‹¨ê³„ ìˆœìˆ˜ ì½”ë“œ (txt íŒŒì¼ ê¸°ë°˜)

    Note: txt íŒŒì¼ì—ëŠ” ì²¨ë¶€ ë¬¸ì„œ ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ attachmentsëŠ” ë¹ˆ ë°°ì—´ì…ë‹ˆë‹¤.
          ì²¨ë¶€ ë¬¸ì„œê°€ í•„ìš”í•œ ê²½ìš° extract_metadata_from_url()ì„ ì‚¬ìš©í•˜ì„¸ìš”.

    Args:
        txt_path: txt íŒŒì¼ ê²½ë¡œ
        api_key: Google API Key
        stage1_model: 1ë‹¨ê³„ ëª¨ë¸ (ê¸°ë³¸: gemini-2.5-pro)
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€ (ê¸°ë³¸: True)

    Returns:
        {
            "meeting_info": {...},
            "chunks": [...],
            "usage": {...}
        }
    """
    logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ì‹œì‘: {txt_path}")

    if verbose:
        print("=" * 100)
        print("í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹±: 1ë‹¨ê³„ Gemini + 2ë‹¨ê³„ ìˆœìˆ˜ ì½”ë“œ")
        print("=" * 100)
        print()

    # txt/md íŒŒì¼ ì½ê¸°
    with open(txt_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # í—¤ë”ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (TXTì™€ MD ë‘˜ ë‹¤ ì§€ì›)
    lines_raw = content.split('\n')

    # ì œëª© ì¶”ì¶œ
    if lines_raw and lines_raw[0].startswith('# '):
        # MD í˜•ì‹: # ì œëª©
        title = lines_raw[0].replace('# ', '').strip()
    elif lines_raw and lines_raw[0].startswith('ì œëª©: '):
        # TXT í˜•ì‹: ì œëª©: xxx
        title = lines_raw[0].replace('ì œëª©: ', '').strip()
    else:
        title = ""

    # URL ì¶”ì¶œ
    url = ""
    for line in lines_raw[:10]:  # ì²« 10ì¤„ì—ì„œ URL ì°¾ê¸°
        if line.startswith('**URL**:'):
            # MD í˜•ì‹: **URL**: https://...
            url = line.replace('**URL**:', '').strip()
            break
        elif line.startswith('URL: '):
            # TXT í˜•ì‹: URL: https://...
            url = line.replace('URL: ', '').strip()
            break

    # ë³¸ë¬¸ë§Œ ì¶”ì¶œ (êµ¬ë¶„ì„  ì´í›„ ë˜ëŠ” í¬ë¡¤ë§ ì‹œê°„ ì´í›„)
    separator_index = content.find('=' * 80)
    if separator_index != -1:
        txt_content = content[separator_index + 80:].strip()
    else:
        # MD íŒŒì¼ì˜ ê²½ìš° í¬ë¡¤ë§ ì‹œê°„ ì´í›„ë¶€í„°
        crawl_time_index = content.find('**í¬ë¡¤ë§ ì‹œê°„**:')
        if crawl_time_index != -1:
            # ë‹¤ìŒ ì¤„ë¶€í„° ì‹œì‘
            remaining = content[crawl_time_index:]
            next_line = remaining.find('\n')
            if next_line != -1:
                txt_content = remaining[next_line + 1:].strip()
            else:
                txt_content = content
        else:
            txt_content = content

    if verbose:
        print(f"ğŸ“„ íŒŒì¼: {txt_path}")
        print(f"ğŸ“ ì œëª©: {title}")
        print(f"ğŸ“ í¬ê¸°: {len(txt_content):,} bytes")
        print()

    # 1ë‹¨ê³„: ì•ˆê±´ ë§¤í•‘ ì¶”ì¶œ (Gemini)
    # ì„ì‹œë¡œ print ì–µì œ
    import sys
    from io import StringIO
    old_stdout = sys.stdout
    if not verbose:
        sys.stdout = StringIO()

    # txtì—ì„œ attachmentsëŠ” ì¶”ì¶œí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
    stage1_result, tokens = extract_agenda_mapping(txt_content, title, url, api_key, [], stage1_model)

    if not verbose:
        sys.stdout = old_stdout

    # 1ë‹¨ê³„ ê²°ê³¼ ì €ì¥ (ë””ë²„ê¹…ìš©)
    stage1_dir = Path("data/result_txt_gemini")
    stage1_dir.mkdir(parents=True, exist_ok=True)
    stage1_filename = Path(txt_path).stem + "_stage1.json"
    stage1_path = stage1_dir / stage1_filename
    with open(stage1_path, 'w', encoding='utf-8') as f:
        json.dump(stage1_result, f, ensure_ascii=False, indent=2)
    logger.info(f"1ë‹¨ê³„ ê²°ê³¼ ì €ì¥: {stage1_path}")

    # 2ë‹¨ê³„: ë°œì–¸ ì¶”ì¶œ (ìˆœìˆ˜ ì½”ë“œ)
    if not verbose:
        sys.stdout = StringIO()

    chunks = parse_with_pure_code(content, stage1_result['agenda_mapping'])

    if not verbose:
        sys.stdout = old_stdout

    # ìµœì¢… ê²°ê³¼ (agenda_mapping í¬í•¨ - attachmentsëŠ” ë¹ˆ ë°°ì—´)
    final_result = {
        "meeting_info": stage1_result['meeting_info'],
        "agenda_mapping": stage1_result['agenda_mapping'],  # â­ ì¶”ê°€: ì•ˆê±´ ë§¤í•‘ (attachments ë¹ˆ ë°°ì—´)
        "chunks": chunks,
        "usage": {
            "stage1_model": stage1_model,
            "stage2_method": "pure_python_code",
            "stage1_tokens": tokens,
            "total_chunks": len(chunks)
        }
    }

    logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ì™„ë£Œ: {len(chunks)}ê°œ ë°œì–¸")

    if verbose:
        print("=" * 100)
        print("âœ… í•˜ì´ë¸Œë¦¬ë“œ íŒŒì‹± ì™„ë£Œ!")
        print("=" * 100)
        print(f"ì´ ë°œì–¸ ìˆ˜: {len(chunks)}ê°œ")
        print(f"Stage 1 í† í°: {tokens['input']:,} + {tokens['output']:,}")
        print(f"Stage 2 ë°©ì‹: ìˆœìˆ˜ Python ì½”ë“œ (ë¹„ìš© 0ì›)")
        print()

    return final_result


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    from dotenv import load_dotenv
    import random
    load_dotenv()

    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        print("âŒ GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return

    # result í´ë”ì˜ ëª¨ë“  txt íŒŒì¼ ì°¾ê¸°
    result_dir = Path("result")
    all_txt_files = list(result_dir.glob("*/meeting_*.txt"))

    if len(all_txt_files) == 0:
        print("âŒ result í´ë”ì— txt íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ëœë¤ìœ¼ë¡œ 3ê°œ ì„ íƒ
    random.seed()  # ë§¤ë²ˆ ë‹¤ë¥¸ ê²°ê³¼
    selected_files = random.sample(all_txt_files, min(3, len(all_txt_files)))

    print("=" * 100)
    print("ğŸ² ëœë¤ìœ¼ë¡œ ì„ íƒëœ íŒŒì¼ 3ê°œ (ìˆ˜ì •ëœ íŒŒì„œ í…ŒìŠ¤íŠ¸)")
    print("=" * 100)
    for i, file in enumerate(selected_files, 1):
        print(f"{i}. {file.parent.name}")
    print()

    success_count = 0
    fail_count = 0

    for idx, txt_path in enumerate(selected_files, 1):
        print("=" * 100)
        print(f"[{idx}/3] {txt_path.parent.name}")
        print("=" * 100)
        print()

        try:
            result = extract_metadata_hybrid(
                txt_path=str(txt_path),
                api_key=api_key,
                stage1_model="gemini-2.5-pro",
                verbose=True
            )

            # ì œëª©ì—ì„œ ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            title = result['meeting_info']['title']
            safe_title = title.replace('/', '_').replace('\\', '_').replace(':', '_').replace('*', '_').replace('?', '_').replace('"', '_').replace('<', '_').replace('>', '_').replace('|', '_')

            # ê²°ê³¼ ì €ì¥ (test_{title}.json)
            output_path = Path("test_results") / f"test_{safe_title}.json"
            output_path.parent.mkdir(exist_ok=True)

            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
            print()

            # í†µê³„
            speakers = set(chunk['speaker'] for chunk in result['chunks'])
            agendas = set(chunk['agenda'] for chunk in result['chunks'])

            print("ğŸ“Š í†µê³„")
            print(f"  - ì´ ë°œì–¸ ìˆ˜: {len(result['chunks'])}ê°œ")
            print(f"  - ë°œì–¸ì: {len(speakers)}ëª…")
            print(f"  - ì•ˆê±´: {len(agendas)}ê°œ")
            print()

            success_count += 1

        except Exception as e:
            logger.error(f"íŒŒì‹± ì‹¤íŒ¨: {txt_path} - {e}")
            print(f"âŒ ì‹¤íŒ¨: {e}")
            print()
            fail_count += 1

    # ìµœì¢… ê²°ê³¼
    print("=" * 100)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("=" * 100)
    print(f"âœ… ì„±ê³µ: {success_count}ê°œ")
    print(f"âŒ ì‹¤íŒ¨: {fail_count}ê°œ")
    print()


if __name__ == "__main__":
    main()
